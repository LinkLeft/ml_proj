{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c88b9a-1d7b-423c-80bb-1fccc925eacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 2020-09-01\n",
      "Processing date: 2020-09-02\n",
      "Processing date: 2020-09-03\n",
      "Processing date: 2020-09-04\n",
      "Processing date: 2020-09-05\n",
      "Processing date: 2020-09-06\n",
      "Processing date: 2020-09-07\n",
      "Processing date: 2020-09-08\n",
      "Processing date: 2020-09-09\n",
      "Processing date: 2020-09-10\n",
      "Processing date: 2020-09-11\n",
      "Processing date: 2020-09-12\n",
      "Processing date: 2020-09-13\n",
      "Processing date: 2020-09-14\n",
      "Processing date: 2020-09-15\n",
      "Processing date: 2020-09-16\n",
      "Processing date: 2020-09-17\n",
      "Processing date: 2020-09-18\n",
      "Processing date: 2020-09-19\n",
      "Processing date: 2020-09-20\n",
      "Processing date: 2020-09-21\n",
      "Processing date: 2020-09-22\n",
      "Processing date: 2020-09-23\n",
      "Processing date: 2020-09-24\n",
      "Processing date: 2020-09-25\n",
      "Processing date: 2020-09-26\n",
      "Processing date: 2020-09-27\n",
      "Processing date: 2020-09-28\n",
      "Processing date: 2020-09-29\n",
      "Processing date: 2020-09-30\n",
      "Processing date: 2020-10-01\n",
      "Processing date: 2020-10-02\n",
      "Processing date: 2020-10-03\n",
      "Processing date: 2020-10-04\n",
      "Processing date: 2020-10-05\n",
      "Processing date: 2020-10-06\n",
      "Processing date: 2020-10-07\n",
      "Processing date: 2020-10-08\n",
      "Processing date: 2020-10-09\n",
      "Processing date: 2020-10-10\n",
      "Processing date: 2020-10-11\n",
      "Processing date: 2020-10-12\n",
      "Processing date: 2020-10-13\n",
      "Processing date: 2020-10-14\n",
      "Processing date: 2020-10-15\n",
      "Processing date: 2020-10-16\n",
      "Processing date: 2020-10-17\n",
      "Processing date: 2020-10-18\n",
      "Processing date: 2020-10-19\n",
      "Processing date: 2020-10-20\n",
      "Processing date: 2020-10-21\n",
      "Processing date: 2020-10-22\n",
      "Processing date: 2020-10-23\n",
      "Processing date: 2020-10-24\n",
      "Processing date: 2020-10-25\n",
      "Processing date: 2020-10-26\n",
      "Processing date: 2020-10-27\n",
      "Processing date: 2020-10-28\n",
      "Processing date: 2020-10-29\n",
      "Processing date: 2020-10-30\n",
      "Processing date: 2020-10-31\n",
      "Processing date: 2020-11-01\n",
      "Processing date: 2020-11-02\n",
      "Processing date: 2020-11-03\n",
      "Processing date: 2020-11-04\n",
      "Processing date: 2020-11-05\n",
      "Processing date: 2020-11-06\n",
      "Processing date: 2020-11-07\n",
      "Processing date: 2020-11-08\n",
      "Processing date: 2020-11-09\n",
      "Processing date: 2020-11-10\n",
      "Processing date: 2020-11-11\n",
      "Processing date: 2020-11-12\n",
      "Processing date: 2020-11-13\n",
      "Processing date: 2020-11-14\n",
      "Processing date: 2020-11-15\n",
      "Processing date: 2020-11-16\n",
      "Processing date: 2020-11-17\n",
      "Processing date: 2020-11-18\n",
      "Processing date: 2020-11-19\n",
      "Processing date: 2020-11-20\n",
      "Processing date: 2020-11-21\n",
      "Processing date: 2020-11-22\n",
      "Processing date: 2020-11-23\n",
      "Processing date: 2020-11-24\n",
      "Processing date: 2020-11-25\n",
      "Processing date: 2020-11-26\n",
      "Processing date: 2020-11-27\n",
      "Processing date: 2020-11-28\n",
      "Processing date: 2020-11-29\n",
      "Processing date: 2020-11-30\n",
      "Processing date: 2020-12-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34478\\AppData\\Local\\Temp\\ipykernel_4568\\2160455119.py:149: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(all_dates_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. Output saved to GraphAutoencoderoutput_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# 设置训练日期范围\n",
    "start_date = datetime(2020, 9, 1)\n",
    "end_date = datetime(2020, 12, 1)  # 先训练十天\n",
    "\n",
    "# 文件路径\n",
    "json_file_path = \"ticker_train_data.json\"\n",
    "xlsx_file_path = \"train_stock_data.xlsx\"\n",
    "output_file_path = \"GraphAutoencoderoutput_data.xlsx\"\n",
    "\n",
    "# 读取 JSON 文件数据\n",
    "with open(json_file_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# 读取 Excel 文件数据\n",
    "df_excel = pd.read_excel(xlsx_file_path)\n",
    "\n",
    "# 定义图自编码器模型\n",
    "class GraphAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, num_nodes):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        # 编码器\n",
    "        self.gc1 = pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        self.gc2 = pyg_nn.GCNConv(hidden_dim, embedding_dim)\n",
    "        # 解码器\n",
    "        self.gc3 = pyg_nn.GCNConv(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_nodes)  # 添加全连接层，输出维度为 num_nodes\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 编码\n",
    "        x = torch.relu(self.gc1(x, edge_index))\n",
    "        embedding = self.gc2(x, edge_index)\n",
    "        # 解码\n",
    "        x = torch.relu(self.gc3(embedding, edge_index))\n",
    "        x = self.fc(x)  # 使用全连接层将每个节点的特征映射到 num_nodes 维度\n",
    "        reconstructed_adj = torch.sigmoid(x @ x.T)  # 计算重建的邻接矩阵\n",
    "        return embedding, reconstructed_adj\n",
    "\n",
    "# 主处理流程\n",
    "all_dates_data = []\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Processing date: {date_str}\")\n",
    "\n",
    "    # 检查当前日期在 JSON 数据中是否存在\n",
    "    date_data = None\n",
    "    for item in json_data:\n",
    "        if item.get(\"Date\") == date_str:\n",
    "            date_data = item\n",
    "            break\n",
    "\n",
    "    if date_data:\n",
    "        # 构建图数据\n",
    "        companies_info = date_data.get(\"Affected Companies\", {})\n",
    "        companies = list(companies_info.keys())\n",
    "        num_nodes = len(companies)\n",
    "        company_to_idx = {company: idx for idx, company in enumerate(companies)}\n",
    "\n",
    "        # 构建节点特征（negative: [1,0,0], positive: [0,1,0], neutral: [0,0,1]）\n",
    "        x = []\n",
    "        for company in companies:\n",
    "            sentiment = companies_info[company]\n",
    "            if sentiment == \"negative\":\n",
    "                x.append([1, 0, 0])\n",
    "            elif sentiment == \"positive\":\n",
    "                x.append([0, 1, 0])\n",
    "            else:  # neutral\n",
    "                x.append([0, 0, 1])\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # 构建边（全连接图）\n",
    "        edge_index = []\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if i != j:\n",
    "                    edge_index.append([i, j])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        # 构建邻接矩阵（作为目标）\n",
    "        adj_matrix = torch.zeros((num_nodes, num_nodes))\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                if i != j:\n",
    "                    adj_matrix[i][j] = 1.0  # 假设所有边都存在，权重为 1.0\n",
    "\n",
    "        # 初始化模型、优化器等\n",
    "        model = GraphAutoencoder(input_dim=3, hidden_dim=16, embedding_dim=32, num_nodes=num_nodes)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        criterion = nn.MSELoss()  # 使用均方误差损失函数\n",
    "\n",
    "        # 训练图自编码器模型\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        embedding, reconstructed_adj = model(x, edge_index)\n",
    "        loss = criterion(reconstructed_adj, adj_matrix)  # 计算重建损失\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 获取节点特征向量（32 维嵌入）\n",
    "        node_features = embedding.detach().numpy()\n",
    "\n",
    "        # 将特征向量添加到 Excel 数据中\n",
    "        # 这里将 \"Date\" 修改为 \"Names Date\"\n",
    "        date_df = df_excel[df_excel[\"Names Date\"].astype(str).str.contains(date_str)].copy()  # 使用 .copy() 创建副本\n",
    "        for idx, row in date_df.iterrows():\n",
    "            ticker = row[\"Ticker Symbol\"]\n",
    "            if ticker in company_to_idx:\n",
    "                feature_idx = company_to_idx[ticker]\n",
    "                feature_vector = node_features[feature_idx]\n",
    "                # 添加特征向量到数据帧中（这里假设数据帧有额外的列来存储特征向量）\n",
    "                for i in range(32):\n",
    "                    date_df.loc[idx, f\"Feature_{i+1}\"] = feature_vector[i]  # 使用 .loc 进行赋值\n",
    "            else:\n",
    "                # 未被提到的公司，特征向量赋值为 0\n",
    "                for i in range(32):\n",
    "                    date_df.loc[idx, f\"Feature_{i+1}\"] = 0.0  # 使用 .loc 进行赋值\n",
    "\n",
    "        all_dates_data.append(date_df)\n",
    "    else:\n",
    "        # 当日没有 JSON 数据，所有公司特征向量赋值为 0\n",
    "        # 这里将 \"Date\" 修改为 \"Names Date\"\n",
    "        date_df = df_excel[df_excel[\"Names Date\"].astype(str).str.contains(date_str)].copy()  # 使用 .copy() 创建副本\n",
    "        if not date_df.empty:\n",
    "            for i in range(32):\n",
    "                date_df[f\"Feature_{i+1}\"] = 0.0\n",
    "            all_dates_data.append(date_df)\n",
    "        else:\n",
    "            # 如果当日没有 JSON 数据且 Excel 中也没有对应日期的数据，则创建一个新的数据框\n",
    "            date_df = pd.DataFrame(columns=df_excel.columns.tolist() + [f\"Feature_{i+1}\" for i in range(32)])\n",
    "            # 假设 \"Names Date\" 列的格式为 \"2020-09-01 00:00:00\"，根据实际情况调整\n",
    "            date_df[\"Names Date\"] = date_str + \" 00:00:00\"\n",
    "            all_dates_data.append(date_df)\n",
    "\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# 合并所有日期的数据并保存到新的 Excel 文件\n",
    "final_df = pd.concat(all_dates_data)\n",
    "final_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data processing complete. Output saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725304a-21ad-4d1f-b98f-72e52d663aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
